{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d22495",
   "metadata": {},
   "source": [
    "### generate 10% as test dataset, 10% validation dataset, and 80% for training dataset. Labels are required for both training dataset and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf70f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load DataFrame\n",
    "df = pd.read_csv('random_90_percent.csv')\n",
    "\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.11, random_state=42)  # Adjust test_size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e854da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15515424",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df.csv', index=False)\n",
    "val_df.to_csv('validation_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf488c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Prepare Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': 'train.csv', 'validation': 'validation.csv'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035db3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add a padding token if it does not exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Verify that the padding token was added\n",
    "print(f\"Padding token: {tokenizer.pad_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde55411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "# Load the model\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=3)\n",
    "\n",
    "# Resize token embeddings layer to accommodate the new special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Tokenize Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('csv', data_files={'train': 'train.csv', 'validation': 'validation.csv'})\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Verify tokenized datasets\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Setup Training Arguments\n",
    "#Define Training Arguments:\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f09fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Train the Model\n",
    "#Initialize Trainer:\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b74c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the dataset has a 'label' column with values 'positive', 'neutral', 'negative'\n",
    "label_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "\n",
    "# Convert labels in the dataset\n",
    "dataset['train'] = dataset['train'].map(lambda example: {'label': label_mapping[example['label']]})\n",
    "dataset['validation'] = dataset['validation'].map(lambda example: {'label': label_mapping[example['label']]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddbe03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets['train']['label'][:5])  # This should print a list of integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b70fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad_token to eos_token (or define a new one)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # or define a new one: tokenizer.pad_token = '<PAD>'\n",
    "\n",
    "# Resize the model embeddings to account for the new padding token\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04fd0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad_token_id in model config\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Define the data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "# Create the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator  # Now it's defined\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04015d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Model:\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ba1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Evaluate the Model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from labels to integers\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.dataframe.iloc[index]['text']\n",
    "        label_str = self.dataframe.iloc[index]['label']\n",
    "        \n",
    "        # Map label string to integer\n",
    "        label = label_mapping[label_str]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ce430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "trainer.evaluate(eval_dataset=tokenized_datasets['validation'], metric_key_prefix=\"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00da830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics  # Add the metrics computation function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d75494",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate(eval_dataset=tokenized_datasets['validation'], metric_key_prefix=\"eval\")\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0dc1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine analyze_predictions to also compute metrics\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_predictions(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    print(\"Label Distribution in True Labels:\", Counter(labels))\n",
    "    print(\"Label Distribution in Predictions:\", Counter(preds))\n",
    "\n",
    "    return compute_metrics(pred)\n",
    "\n",
    "# Update the Trainer initialization\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=analyze_predictions  # Pass it here\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "trainer.evaluate(eval_dataset=tokenized_datasets['validation'], metric_key_prefix=\"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8: Inference\n",
    "\n",
    "inputs = tokenizer(\"This product is not fantastic!\", return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=-1)\n",
    "print(outputs)\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e061e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Save and Load the Model\n",
    "#Save the Model and Tokenizer:\n",
    "\n",
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c6eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Model and Tokenizer Later:\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('./saved_model')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./saved_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf63dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9465b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a confusion matrix for the GPT model.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "\n",
    "# Step 1: Get predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tokenized_datasets['validation']:\n",
    "    inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Step 2: Convert model outputs (logits) to predicted class labels\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    predictions.extend(predicted_class)\n",
    "    true_labels.append(batch['label'])  # Use append instead of extend\n",
    "\n",
    "# Step 3: Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions, labels=[0, 1, 2])  # For 'negative', 'neutral', 'positive'\n",
    "\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataloader = DataLoader(tokenized_datasets['validation'], batch_size=32, shuffle=True)\n",
    "for batch in dataloader:\n",
    "    input_ids = batch['input_ids']  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91363d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf48178",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets['validation'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81edc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, pad_value=0):\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded = [seq + [pad_value] * (max_length - len(seq)) for seq in sequences]\n",
    "    return torch.tensor(padded, dtype=torch.long)\n",
    "\n",
    "# Pad manually\n",
    "input_ids_tensor = pad_sequences(input_ids).to(model.device)\n",
    "attention_mask_tensor = pad_sequences(attention_mask).to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6995a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Print the types and lengths for debugging\n",
    "print(f\"Type of input_ids: {type(input_ids)}, Length: {len(input_ids)}\")\n",
    "print(f\"Type of attention_mask: {type(attention_mask)}, Length: {len(attention_mask)}\")\n",
    "\n",
    "try:\n",
    "    # Ensure input_ids is a list of tensors\n",
    "    if isinstance(input_ids, list) and all(isinstance(i, torch.Tensor) for i in input_ids):\n",
    "        # Pad the sequences if they are of varying lengths\n",
    "        input_ids_tensor = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        raise ValueError(\"input_ids is not a list of tensors or contains non-tensor elements.\")\n",
    "    \n",
    "    print(f\"Padded input_ids_tensor shape: {input_ids_tensor.shape}\")\n",
    "\n",
    "    # Ensure attention_mask is a list of tensors\n",
    "    if isinstance(attention_mask, list) and all(isinstance(i, torch.Tensor) for i in attention_mask):\n",
    "        # Pad the attention masks similarly\n",
    "        attention_mask_tensor = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        raise ValueError(\"attention_mask is not a list of tensors or contains non-tensor elements.\")\n",
    "\n",
    "    print(f\"Padded attention_mask_tensor shape: {attention_mask_tensor.shape}\")\n",
    "\n",
    "    # Move tensors to the model's device (CPU or GPU)\n",
    "    input_ids_tensor = input_ids_tensor.to(model.device)\n",
    "    attention_mask_tensor = attention_mask_tensor.to(model.device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids_tensor, attention_mask=attention_mask_tensor)\n",
    "    print(outputs)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume input_ids and attention_mask are provided as before\n",
    "# input_ids = ...\n",
    "# attention_mask = ...\n",
    "\n",
    "# Check if input_ids is a list of tensors\n",
    "if isinstance(input_ids, list) and isinstance(input_ids[0], torch.Tensor):\n",
    "    # Stack tensors along a new dimension\n",
    "    input_ids = torch.stack(input_ids)\n",
    "elif isinstance(input_ids, list):\n",
    "    # Convert directly to tensor if it's a flat list of integers\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "# Check if attention_mask is a list of tensors\n",
    "if isinstance(attention_mask, list) and isinstance(attention_mask[0], torch.Tensor):\n",
    "    # Stack tensors along a new dimension\n",
    "    attention_mask = torch.stack(attention_mask)\n",
    "elif isinstance(attention_mask, list):\n",
    "    # Convert directly to tensor if it's a flat list of integers\n",
    "    attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "\n",
    "# Now input_ids and attention_mask should be tensors\n",
    "print(\"input_ids shape:\", input_ids.shape)\n",
    "print(\"attention_mask shape:\", attention_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign eos_token as pad_token or add [PAD] token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Option 1\n",
    "   \n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Now proceed with data processing\n",
    "for batch in tokenized_datasets['validation']:\n",
    "    inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31abf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "dataset for evaluation\n",
    "\n",
    "# Step 1: Get predicted probabilities and true labels\n",
    "predicted_probs = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tokenized_datasets['validation']:\n",
    "    inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Step 2: Convert logits to probabilities using softmax\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    predicted_probs.extend(probs)\n",
    "    \n",
    "    # Corrected label handling based on whether batch['label'] is int or list\n",
    "    if isinstance(batch['label'], int):  # If it's an integer\n",
    "        true_labels.append(batch['label'])\n",
    "    else:  # If it's a list of labels\n",
    "        true_labels.extend(batch['label'])\n",
    "\n",
    "# Step 3: Convert true labels to one-hot encoding for multiclass ROC-AUC\n",
    "true_labels_one_hot = np.zeros((len(true_labels), 3))  # Assuming 3 classes\n",
    "for i, label in enumerate(true_labels):\n",
    "    true_labels_one_hot[i, label] = 1\n",
    "\n",
    "# Step 4: Compute the ROC-AUC score using one-vs-rest approach\n",
    "roc_auc = roc_auc_score(true_labels_one_hot, np.array(predicted_probs), multi_class='ovr')\n",
    "\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51688d8",
   "metadata": {},
   "source": [
    "### the following is to predict on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a119789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame\n",
    "df_test = pd.read_csv('df_remaining_10.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# Load trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./saved_model')\n",
    "model = GPT2ForSequenceClassification.from_pretrained('./saved_model')\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def predict_sentiment(texts, tokenizer, model):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # Run the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get the logits and apply softmax\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1).numpy()\n",
    "        \n",
    "        # Get the predicted label\n",
    "        predicted_label = np.argmax(probabilities, axis=1)[0]\n",
    "        sentiment_score = probabilities[0][predicted_label]\n",
    "        confidence = np.max(probabilities)\n",
    "\n",
    "        results.append({\n",
    "            'label': predicted_label,\n",
    "            'sentiment_score': sentiment_score,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "\n",
    "local_model_path = './saved_model'\n",
    "\n",
    "# Load the model and tokenizer from local directory\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(local_model_path)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(local_model_path)\n",
    "\n",
    "# Example usage\n",
    "model.eval()\n",
    "text = \"I use this product!\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1).detach().numpy()\n",
    "predicted_label = np.argmax(probabilities, axis=1)[0]\n",
    "confidence = np.max(probabilities)\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label}, Confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the prediction function\n",
    "predictions = predict_sentiment(df_test['clean_text'], tokenizer, model)\n",
    "\n",
    "# Create a new DataFrame with predictions\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "# Concatenate the original test DataFrame with the predictions\n",
    "result_df = pd.concat([df_test, predictions_df], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "result_df.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping from numeric labels to categorical labels\n",
    "label_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# Convert the numeric labels to categorical labels\n",
    "result_df['label'] = result_df['label'].map(label_mapping)\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37597f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.drop(columns=['label_2'])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "result_df.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef337f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = pd.read_csv('random_90_percent.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfa124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the random 90% DataFrame with the predictions 10%\n",
    "whole_df = pd.concat([df, result_df], axis=0)\n",
    "whole_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df.to_csv('whole_df_916.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
